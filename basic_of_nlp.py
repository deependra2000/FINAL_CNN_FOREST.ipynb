# -*- coding: utf-8 -*-
"""Basic Of NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DoApvYpAM8O-Cp0HvylVzdDWW9stWp-h
"""

import nltk
nltk.download('punkt')
nltk.download('wordnet')

sentence_data = "This is first sentence. Here we go! Let check last sentence"

"""## Tokenization:
- It is a process by which body of text is devided into smaller parts called tokens

### 1. Sentence Tokenization
"""

sentences = nltk.sent_tokenize(sentence_data)
print (sentences)

"""### 2. Word Tokenization"""

words = nltk.word_tokenize(sentence_data)
print (words)

from nltk.stem.porter import PorterStemmer

text = "We are writting this sentence to check stemming. We will go one by one further"
words = nltk.word_tokenize(text)
print("Before Stemming ==> ", words)
stemmer = PorterStemmer()
words = [stemmer.stem(word) for word in words]
print("After Stemming==> ",words)

from nltk.stem import WordNetLemmatizer

text = "We have written these sentences to check stemming. This is just for programmers and to give you examples"
words = nltk.word_tokenize(text)
print("Before Lemmitization ==> ", words)
lemmitizer = WordNetLemmatizer()
words = [lemmitizer.lemmatize(word) for word in words]
print("After Lemmitization ==> ", words)

"""## StopWords
Stopwords are the words in any language which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence.
- e.g: is, a, the, this, will, for, where, when, to, and
"""

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

print(stopwords.words('english'))

from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

text = "We have written these sentences to check stemming. This is just for programmers and to give you examples"
words = nltk.word_tokenize(text)
print("Before Lemmitization==>", words)

lemmitizer = WordNetLemmatizer()
words = [lemmitizer.lemmatize(word) for word in words if word not in stopwords.words('english')]
print("After Lemmitization==>", words)

from sklearn.feature_extraction.text import CountVectorizer

corpus = ["He is a good boy", "She is a good girl", "Boy and Girl both are good"]
print("old corpus==>", corpus)

new_corpus=[]
for sentence in corpus:
  words = nltk.word_tokenize(sentence)
  words = [word for word in words if word not in (stopwords.words('english'))]
  new_corpus.append(" ".join(words))
print("new_corpus==>",new_corpus)

vectorizer = CountVectorizer()
x = vectorizer.fit_transform(new_corpus).toarray()
print(x)

print(vectorizer.get_feature_names_out())

import pandas as pd
df = pd.DataFrame(x, columns = vectorizer.get_feature_names_out())
df.head()



from sklearn.feature_extraction.text import CountVectorizer

corpus = ["He is a good boy", "She is a good girl", "Boy and Girl both are good"]
print("old corpus==>", corpus)

new_corpus=[]
for sentence in corpus:
  words = nltk.word_tokenize(sentence)
  words = [word for word in words if word not in (stopwords.words('english'))]
  new_corpus.append(" ".join(words))
print("new_corpus==>",new_corpus)

## For unigram

vectorizer = CountVectorizer(ngram_range=(1,1))
x = vectorizer.fit_transform(new_corpus).toarray()
print(x, end= "\n\n")
print(vectorizer.get_feature_names_out())

# For Biagrams

vectorizer = CountVectorizer(ngram_range=(2,2))
x = vectorizer.fit_transform(new_corpus).toarray()
print(x, end= "\n\n")
print(vectorizer.get_feature_names_out())

# For unigram and biagram

vectorizer = CountVectorizer(ngram_range=(1,2))
x = vectorizer.fit_transform(new_corpus).toarray()
# print(x, end= "\n\n")
print(vectorizer.get_feature_names_out())

df = pd.DataFrame(x, columns = vectorizer.get_feature_names_out())
df.head()



"""# TF-IDF"""

corpus = ["He is a good boy", "She is a good girl", "Boy and Girl both are good"]
print("old corpus==>", corpus)

new_corpus=[]
for sentence in corpus:
  words = nltk.word_tokenize(sentence)
  words = [word.lower() for word in words]
  words = [word for word in words if word not in (stopwords.words('english'))]
  new_corpus.append(" ".join(words))
print("new_corpus==>",new_corpus)

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()

x = vectorizer.fit_transform(new_corpus).toarray()
print(x, end= "\n\n")
print(vectorizer.get_feature_names_out())

import pandas as pd
df =pd.DataFrame(x, columns=vectorizer.get_feature_names_out())
df